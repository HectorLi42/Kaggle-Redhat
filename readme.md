## Kaggle competation: Red hat

### log

===
### 2016-09-11

* finish data cleaning
* train a random forest model
* tune parameter `n_estimator`, `max_depth` with `GridSearchCV`

do first submit, score = 0.96,rank= 1428 

#### what to do in the future

* implement xgboost under linux system
* try feature selection

「以上」

### 2016-09-12

* feature selection/engineering is much more powerful than using an ensemble algorithm
* score imporved to 0.987, rank = 885